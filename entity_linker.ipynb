{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c252ef-6591-48d7-b7ab-1dd520a02d17",
   "metadata": {},
   "source": [
    "# Benchmark Solution for the SNOMED CT Entity Linking Challenge\n",
    "\n",
    "Welcome! This blog post contains code for training the benchmark entity linking model for the [SNOMED CT Entity Linking Challenge](https://www.drivendata.org/competitions/258/competition-snomed-ct/). You can find the code and instructions for reproducing this notebook in [this repository](https://github.com/BayesRulez/snomed_el_baseline_model).\n",
    "\n",
    "## Background\n",
    "\n",
    "Much of the world's healthcare data is stored in free-text documents, usually clinical notes taken by doctors. This unstructured data can be challenging to analyze and extract meaningful insights from. However, by applying a standardized terminology like SNOMED CT, healthcare organizations can convert this free-text data into a structured format that can be readily analyzed by computers, in turn stimulating the development of new medicines, treatment pathways, and better patient outcomes.\n",
    "\n",
    "One way to analyze clinical notes is to identify and label the portions of each note that correspond to specific medical concepts. This process is called entity linking because it involves identifying candidate spans in the unstructured text (the entities) and linking them to a particular concept in a knowledge base of medical terminology.\n",
    "\n",
    "However, clinical entity linking is hard!  Medical notes are often rife with abbreviations (some of them context-dependent) and assumed knowledge. Furthermore, the target knowledge bases can easily include hundreds of thousands of concepts, many of which occur infrequently leading to a “long tail” effect in the distribution of concepts.\n",
    "\n",
    "The objective of the competition is to link spans of text in clinical notes with specific topics in the SNOMED CT clinical terminology. In this post, we build a straightforward entity linking model and prepare it for submission.  \n",
    "\n",
    "## Benchmark architecture overview\n",
    "\n",
    "Typically, an entity linker contains two components:\n",
    "\n",
    "- A \"Clinical Entity Recognizer\" (CER) that is responsible for detecting candidate clinical entities from within a text.\n",
    "- A \"Linker\" that is responsible for \"linking\" entities detected by the CER to concepts in the knowledge base.  Often (as here) the linker's tasks are split into two steps:\n",
    "    - In the Candidate Generation step, the Linker retrieves a handful of candidate concepts that it thinks may match to the entity.\n",
    "    - In the Candidate Selection step, the Linker selects the best candidate.\n",
    "\n",
    "For this benchmark solution, we will finetune pre-trained base models for each of these components by using the provided training data for the challenge as well as SNOMED CT. We also provide an option to use [LoRA](https://huggingface.co/docs/diffusers/main/en/training/lora) (Low-Rank Adaptation of Large Language Models) to reduce resources required for training and to speed up CER model training.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "If you'd like to be able to reproduce this notebook or expand upon it for your own submissions, you'll need a few things:\n",
    "\n",
    "- A GPU machine with at least 24GB of VRAM\n",
    "    - Note: It's possible to use this notebook on machines with less VRAM, but you may need to use a different base model for the CER like `deberta-v3-base`, use `LoRA` or an equivalent low-rank LLM adaptation, train with mixed precision by setting `fp16=True` in the `TrainingArguments`, and/or decrease the batch size.\n",
    "- A conda environment that matches the environment provided in [`environment-gpu.yml`](https://github.com/drivendataorg/snomed-ct-entity-linking-runtime/blob/main/runtime/environment-gpu.yml) or [`conda-lock-gpu.yml`](https://github.com/drivendataorg/snomed-ct-entity-linking-runtime/blob/main/runtime/conda-lock-gpu.yml) from the challenge [runtime repository](https://github.com/drivendataorg/snomed-ct-entity-linking-runtime)\n",
    "- A clone of the [benchmark repository](https://github.com/BayesRulez/snomed_el_baseline_model) to install additional requirements (specified in `requirements.txt`) as well as leverage utilities for SNOMED CT (in `snomed_graph.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2596f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dec53e4-02be-4e99-9165-091942597bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 22:27:10.988159: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-21 22:27:11.013948: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-21 22:27:11.013994: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-21 22:27:11.014002: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-21 22:27:11.018197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import dill as pickle\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from ipymarkup import show_span_line_markup\n",
    "from more_itertools import chunked\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
    "from sentence_transformers import InputExample, SentenceTransformer, losses, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    DebertaV2ForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModel,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from snomed_graph import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c3b83a-bab1-453c-9321-ecfd30f993a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42  # For reproducibility\n",
    "max_seq_len = 512  # Maximum sequence length for (BERT-based) encoders\n",
    "# cer_model_id = \"microsoft/deberta-v3-large\"  # Base model for Clinical Entity Recogniser\n",
    "cer_model_id = \"michiyasunaga/BioLinkBERT-base\"  # Base model for Clinical Entity Recogniser\n",
    "kb_embedding_model_id = (\"sentence-transformers/all-MiniLM-L6-v2\") # base model for concept encoder\n",
    "use_LoRA = False  # Whether to use a LoRA to fine-tune the CER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51114be-b3f8-4586-8c57-a4072b32f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d4eb0-8721-4aff-b908-30fee696023c",
   "metadata": {},
   "source": [
    "# 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6a22c4-617a-42e0-b9b1-dcc5d2c6b685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 notes loaded.\n"
     ]
    }
   ],
   "source": [
    "notes_df = pd.read_csv(\"data/training_notes.csv\").set_index(\"note_id\")\n",
    "print(f\"{notes_df.shape[0]} notes loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7506cc-d532-49f2-89b7-aaea55d06c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51574 annotations loaded.\n",
      "5336 unique concepts seen.\n",
      "204 unique notes seen.\n"
     ]
    }
   ],
   "source": [
    "annotations_df = pd.read_csv(\"data/train_annotations.csv\").set_index(\"note_id\")\n",
    "print(f\"{annotations_df.shape[0]} annotations loaded.\")\n",
    "print(f\"{annotations_df.concept_id.nunique()} unique concepts seen.\")\n",
    "print(f\"{annotations_df.index.nunique()} unique notes seen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f64137-236d-43e4-abda-4ba3e290c527",
   "metadata": {},
   "source": [
    "## 1.1 Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e57731-4b49-41b3-8bff-45e2ab76e642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 44075 total annotations in the training set.\n",
      "There are 7499 total annotations in the test set.\n",
      "There are 4924 distinct concepts in the training set.\n",
      "There are 1799 distinct concepts in the test set.\n",
      "There are 172 notes in the training set.\n",
      "There are 32 notes in the test set.\n"
     ]
    }
   ],
   "source": [
    "training_notes_df, test_notes_df = train_test_split(\n",
    "    notes_df, test_size=32, random_state=random_seed\n",
    ")\n",
    "training_annotations_df = annotations_df.loc[training_notes_df.index]\n",
    "test_annotations_df = annotations_df.loc[test_notes_df.index]\n",
    "\n",
    "print(\n",
    "    f\"There are {training_annotations_df.shape[0]} total annotations in the training set.\"\n",
    ")\n",
    "print(f\"There are {test_annotations_df.shape[0]} total annotations in the test set.\")\n",
    "print(\n",
    "    f\"There are {training_annotations_df.concept_id.nunique()} distinct concepts in the training set.\"\n",
    ")\n",
    "print(\n",
    "    f\"There are {test_annotations_df.concept_id.nunique()} distinct concepts in the test set.\"\n",
    ")\n",
    "print(f\"There are {training_notes_df.shape[0]} notes in the training set.\")\n",
    "print(f\"There are {test_notes_df.shape[0]} notes in the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bdcdff-cabd-4012-a85a-90fd102bdd32",
   "metadata": {},
   "source": [
    "# 2. Train the CER model\n",
    "\n",
    "This will be a token classifier, based on the widely-used BERT architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731650a7-fca9-4cf1-be5a-aae9ed42e3b5",
   "metadata": {},
   "source": [
    "## 2.1 Define the token types\n",
    "\n",
    "A token classifier is typically looking to tag tokens according to the part of speech or entity type.  We have quite a simple task here: locate tokens that are part of clinical entities.  We define the following token labels:\n",
    "\n",
    "- *O*.  This token is not part of an entity.\n",
    "- *B-clinical_entity*. This token is the beginning (first part of the first word) of a clinical entity.\n",
    "- *I-clinical_entity*. This token is inside a clinical entity - i.e. not the first word but a subsequent word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ab477a-c300-4368-8976-a4addc397041",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"O\": 0, \"B-clinical_entity\": 1, \"I-clinical_entity\": 2}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ead1df-a752-4316-99fd-45650e52c8e7",
   "metadata": {},
   "source": [
    "## 2.2 Load a tokenizer\n",
    "\n",
    "We'll use the tokenizer for our chosen NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ca0f45-4afa-4dbd-9c84-25db15d78d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cer_model_id, model_max_length=max_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf1bc37-6fb0-4207-9a7f-d6a690d513e1",
   "metadata": {},
   "source": [
    "## 2.3 Construct training and test datasets for the CER model\n",
    "\n",
    "The annotation dataset contains tuples of the form `(note_id, concept_id, start, end)`.\n",
    "\n",
    "To create a dataset for the token classifier we need to make two transformations to the data:\n",
    "\n",
    "1. We have to split the discharge notes into chunks of 512 characters (the input dimension for BERT-based models).\n",
    "2. We have to tokenize the discharge notes and determine which of the resulting tokens fall within the span of an annotation according to the `label2id` map defined above.\n",
    "\n",
    "We will create a dataset consisting of 512-token chunks, along with a length-512 vector flagging the tokens which appear within an annotation.\n",
    "\n",
    "One further consideration is that the tokenizer will tokenize to a sub-word level.  For example, this tokenizer will split the word `tokenization` into two sub-words: `__token` and `ization`.  We will always flag the first token of each word with the appropriate entity type (\"B\", \"I\" or \"O\") but need to decide how to flag subsequent sub-words.  One way is to flag these with a `-100` value, which is interpreted used by `pytorch` loss functions as \"ignore this value\".  This involves complicating the alignment logic, however.  Instead, the approach taken below is to flag all subwords with the appropriate \"I\" or \"B\" label.  (The tokenizer offers a handy `word_ids()` function which we can use to determine whether a particular token represents the start of a new word or the continuation of the previous word.)\n",
    "\n",
    "The logic for the CER tokenizer is therefore as follows:\n",
    "\n",
    "- First token of the first word within an annotation: `B-clinical_entity`\n",
    "- First token a subsequent word within an annotation: `I-clinical_entity`\n",
    "- First token of a word not within an annotation: `O`\n",
    "- Special token ([CLS], [SEP]): `-100`\n",
    "\n",
    "The first token of an input to a BERT-based model must be the classificiation (`[CLS]`) token and the last must be the separator (`[SEP]`).  We add these manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9220c9e7-e516-4a43-a67a-68e117ce2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through the annotation spans for a given note.  When they're exhausted,\n",
    "# return (1000000, 1000000).  This will avoid a StopIteration exception.\n",
    "\n",
    "def get_annotation_boundaries(note_id, annotations_df):\n",
    "    for row in annotations_df.loc[note_id].itertuples():\n",
    "        yield row.start, row.end, row.concept_id\n",
    "    yield 1000000, 1000000, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36dac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {\n",
    "    \"O\": 0,\n",
    "    \"B-clinical_entity\": 0,\n",
    "    \"I-clinical_entity\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b802c41-c760-4c2d-ab91-dca044b2e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_dataset(notes_df, annotations_df):\n",
    "    for row in notes_df.itertuples():\n",
    "        tokenized = cer_tokenizer(\n",
    "            row.text,\n",
    "            return_offsets_mapping=False,  # Avoid misalignments due to destructive tokenization\n",
    "            return_token_type_ids=False,  # We're going to construct these below\n",
    "            return_attention_mask=False,  # We'll construct this by hand\n",
    "            add_special_tokens=False,  # We'll add these by hand\n",
    "            truncation=False,  # We'll chunk the notes ourselves\n",
    "        )\n",
    "\n",
    "        # Prime the annotation generator and fetch the token <-> word_id map\n",
    "        annotation_boundaries = get_annotation_boundaries(row.Index, annotations_df)\n",
    "        ann_start, ann_end, concept_id = next(annotation_boundaries)\n",
    "        word_ids = tokenized.word_ids()\n",
    "\n",
    "        # The offsets_mapping returned by the tokenizer will be misaligned vs the original text.\n",
    "        # This is due to the fact that the tokenization scheme is destructive, for example it\n",
    "        # drops spaces which cannot be recovered when decoding the inputs.\n",
    "        # In the following code snippet we create an offset mapping which is aligned with the\n",
    "        # original text; hence we can accurately locate the annotations and match them to the\n",
    "        # tokens.\n",
    "        global_offset = 0\n",
    "        global_offset_mapping = []\n",
    "\n",
    "        for input_id in tokenized[\"input_ids\"]:\n",
    "            token = cer_tokenizer.decode(input_id)\n",
    "            pos = row.text[global_offset:].find(token)\n",
    "            start = global_offset + pos\n",
    "            end = global_offset + pos + len(token)\n",
    "            global_offset = end\n",
    "            global_offset_mapping.append((start, end))\n",
    "\n",
    "        # Note the max_seq_len - 2.\n",
    "        # This is because we will have to add [CLS] and [SEP] tokens once we're done.\n",
    "        it = zip(\n",
    "            chunked(tokenized[\"input_ids\"], max_seq_len - 2),\n",
    "            chunked(global_offset_mapping, max_seq_len - 2),\n",
    "            chunked(word_ids, max_seq_len - 2),\n",
    "        )\n",
    "\n",
    "        # Since we are chunking the discharge notes, we need to maintain the start and\n",
    "        # end character index for each chunk so that we can align the annotations for\n",
    "        # chunks > 1\n",
    "        chunk_start_idx = 0\n",
    "        chunk_end_idx = 0\n",
    "\n",
    "        for chunk_id, chunk in enumerate(it):\n",
    "            input_id_chunk, offset_mapping_chunk, word_id_chunk = chunk\n",
    "            token_type_chunk = list()\n",
    "            concept_id_chunk = list()\n",
    "            prev_word_id = -1\n",
    "            concept_word_number = 0\n",
    "            chunk_start_idx = chunk_end_idx\n",
    "            chunk_end_idx = offset_mapping_chunk[-1][1]\n",
    "\n",
    "            for offsets, word_id in zip(offset_mapping_chunk, word_id_chunk):\n",
    "                token_start, token_end = offsets\n",
    "\n",
    "                # Check whether we need to fetch the next annotation\n",
    "                if token_start >= ann_end:\n",
    "                    ann_start, ann_end, concept_id = next(annotation_boundaries)\n",
    "                    concept_word_number = 0\n",
    "\n",
    "                # Check whether the token's position overlaps with the next annotation\n",
    "                if token_start < ann_end and token_end > ann_start:\n",
    "                    if prev_word_id != word_id:\n",
    "                        concept_word_number += 1\n",
    "\n",
    "                    # If so, annotate based on the word number in the concept\n",
    "                    if concept_word_number == 1:\n",
    "                        token_type_chunk.append(label2id[\"B-clinical_entity\"])\n",
    "                        counts[\"B-clinical_entity\"] += 1\n",
    "                    else:\n",
    "                        token_type_chunk.append(label2id[\"I-clinical_entity\"])\n",
    "                        counts[\"I-clinical_entity\"] += 1\n",
    "\n",
    "                    # Add the SCTID (we'll use this later to train the Linker)\n",
    "                    concept_id_chunk.append(concept_id)\n",
    "\n",
    "                # Not part of an annotation\n",
    "                else:\n",
    "                    token_type_chunk.append(label2id[\"O\"])\n",
    "                    counts[\"O\"] += 1\n",
    "                    concept_id_chunk.append(None)\n",
    "\n",
    "                prev_word_id = word_id\n",
    "\n",
    "            # Manually adding the [CLS] and [SEP] tokens.\n",
    "            token_type_chunk = [-100] + token_type_chunk + [-100]\n",
    "            input_id_chunk = (\n",
    "                [cer_tokenizer.cls_token_id]\n",
    "                + input_id_chunk\n",
    "                + [cer_tokenizer.sep_token_id]\n",
    "            )\n",
    "            attention_mask_chunk = [1] * len(input_id_chunk)\n",
    "            offset_mapping_chunk = (\n",
    "                [(None, None)] + offset_mapping_chunk + [(None, None)]\n",
    "            )\n",
    "            concept_id_chunk = [None] + concept_id_chunk + [None]\n",
    "\n",
    "            yield {\n",
    "                # These are the fields we need\n",
    "                \"note_id\": row.Index,\n",
    "                \"input_ids\": input_id_chunk,\n",
    "                \"attention_mask\": attention_mask_chunk,\n",
    "                \"labels\": token_type_chunk,\n",
    "                # These fields are helpful for debugging\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"chunk_span\": (chunk_start_idx, chunk_end_idx),\n",
    "                \"offset_mapping\": offset_mapping_chunk,\n",
    "                \"text\": row.text[chunk_start_idx:chunk_end_idx],\n",
    "                \"concept_ids\": concept_id_chunk,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "382fcd88-0ccd-4fd9-bffd-e9b45cbb62f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1569 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569\n",
      "3327\n",
      "1654\n",
      "582\n",
      "1899\n",
      "2609\n",
      "1603\n",
      "1512\n",
      "2521\n",
      "4364\n",
      "2900\n",
      "1864\n",
      "2260\n",
      "3103\n",
      "2150\n",
      "2778\n",
      "3286\n",
      "2251\n",
      "2296\n",
      "2620\n",
      "1337\n",
      "2171\n",
      "1967\n",
      "3351\n",
      "2226\n",
      "3705\n",
      "3121\n",
      "1875\n",
      "2411\n",
      "2486\n",
      "2345\n",
      "2288\n",
      "2371\n",
      "2524\n",
      "1630\n",
      "2381\n",
      "2905\n",
      "2263\n",
      "903\n",
      "2040\n",
      "5134\n",
      "3558\n",
      "3665\n",
      "2766\n",
      "2350\n",
      "2289\n",
      "1413\n",
      "2410\n",
      "1810\n",
      "3680\n",
      "3160\n",
      "1737\n",
      "1302\n",
      "1213\n",
      "2018\n",
      "1494\n",
      "1922\n",
      "2809\n",
      "2751\n",
      "1488\n",
      "3808\n",
      "1639\n",
      "1692\n",
      "2444\n",
      "3358\n",
      "2671\n",
      "2136\n",
      "3014\n",
      "3215\n",
      "3094\n",
      "2039\n",
      "2145\n",
      "2924\n",
      "2084\n",
      "2909\n",
      "3171\n",
      "2735\n",
      "1326\n",
      "1372\n",
      "5510\n",
      "921\n",
      "2455\n",
      "3438\n",
      "3138\n",
      "1881\n",
      "2379\n",
      "2907\n",
      "1978\n",
      "1797\n",
      "1956\n",
      "1905\n",
      "4714\n",
      "1992\n",
      "1949\n",
      "2252\n",
      "2866\n",
      "1818\n",
      "1592\n",
      "2645\n",
      "2953\n",
      "1462\n",
      "3318\n",
      "3004\n",
      "2208\n",
      "2177\n",
      "2577\n",
      "2633\n",
      "2800\n",
      "4910\n",
      "2435\n",
      "2767\n",
      "2880\n",
      "1676\n",
      "1545\n",
      "2036\n",
      "1944\n",
      "2433\n",
      "4319\n",
      "2310\n",
      "2478\n",
      "2284\n",
      "2498\n",
      "1341\n",
      "2216\n",
      "1678\n",
      "2817\n",
      "3620\n",
      "2568\n",
      "3251\n",
      "2643\n",
      "3207\n",
      "797\n",
      "1522\n",
      "2428\n",
      "1531\n",
      "1092\n",
      "1744\n",
      "2323\n",
      "3577\n",
      "3046\n",
      "2753\n",
      "3725\n",
      "1601\n",
      "2424\n",
      "2256\n",
      "1781\n",
      "1577\n",
      "2889\n",
      "5081\n",
      "1873\n",
      "2824\n",
      "2605\n",
      "2072\n",
      "1755\n",
      "2642\n",
      "3992\n",
      "1816\n",
      "1704\n",
      "1813\n",
      "1580\n",
      "1541\n",
      "1905\n",
      "2188\n",
      "2111\n",
      "2108\n",
      "1899\n",
      "2013\n",
      "2801\n",
      "1055\n",
      "1682\n",
      "3245\n",
      "2366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['note_id', 'input_ids', 'attention_mask', 'labels', 'chunk_id', 'chunk_span', 'offset_mapping', 'text', 'concept_ids'],\n",
       "    num_rows: 900\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can ignore the \"Token indices sequence length is longer than the specified maximum sequence length\"\n",
    "# warning because we are chunking by hand.\n",
    "# 1629\n",
    "# 3394\n",
    "# 1686\n",
    "# 613\n",
    "# 1940\n",
    "train = pd.DataFrame(\n",
    "    list(generate_ner_dataset(training_notes_df, training_annotations_df))\n",
    ")\n",
    "train = Dataset.from_pandas(train)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c02046-025b-4c13-9359-78f11b398664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305\n",
      "1342\n",
      "1675\n",
      "2441\n",
      "1436\n",
      "2865\n",
      "1878\n",
      "1822\n",
      "1504\n",
      "1510\n",
      "2157\n",
      "2601\n",
      "2176\n",
      "1192\n",
      "3233\n",
      "2473\n",
      "2324\n",
      "3276\n",
      "2408\n",
      "1955\n",
      "1938\n",
      "866\n",
      "2486\n",
      "2931\n",
      "1650\n",
      "3244\n",
      "1974\n",
      "2696\n",
      "4645\n",
      "1184\n",
      "1565\n",
      "2277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['note_id', 'input_ids', 'attention_mask', 'labels', 'chunk_id', 'chunk_span', 'offset_mapping', 'text', 'concept_ids'],\n",
       "    num_rows: 150\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame(list(generate_ner_dataset(test_notes_df, test_annotations_df)))\n",
    "test = Dataset.from_pandas(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad947239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 484014, 'B-clinical_entity': 26, 'I-clinical_entity': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "030d7dd8-62f6-4d1b-bcda-64157fce7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data collator handles batching for us.\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=cer_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07519dc5-5217-43af-8246-9808f3b6bc3f",
   "metadata": {},
   "source": [
    "## 2.4 Define some training metrics for the fine-tuning run\n",
    "\n",
    "It's always easier to be able to track some meaningful performance metrics during a training run, rather than simple watching a cross-entropy loss function change.  This is a standard, boilerplate function taken directly from a HuggingFace tutorial that is useful for any classifier fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56d6d2b0-2c08-444b-83c4-56684577a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4cb30-2538-47b0-b3d0-985f40c22330",
   "metadata": {},
   "source": [
    "## 2.5 Define and train the model\n",
    "\n",
    "The `deberta-v3-large` model (model card: https://huggingface.co/microsoft/deberta-v3-large) has 304M parameters.  To speed up the fine-tuning can use a LoRA, which will greatly reduce the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93a17fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34f6d8b3-78be-48eb-914f-dcbcfa9b347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziggy/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# cer_model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "#     cer_model_id, num_labels=3, id2label=id2label, label2id=label2id\n",
    "# )\n",
    "\n",
    "cer_model = BertForTokenClassification.from_pretrained(\n",
    "    'michiyasunaga/BioLinkBERT-large', num_labels=3, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "if use_LoRA:\n",
    "    lora_config = LoraConfig(\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        task_type=\"TOKEN_CLS\",\n",
    "    )\n",
    "\n",
    "    cer_model = get_peft_model(cer_model, lora_config)\n",
    "\n",
    "    cer_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a8f02d5-9817-4ee8-a787-563099dd730d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eece45e04fc4b098ff196eb09e82ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1702400366987/work/aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [142,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 26\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/temp/cer_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     seed\u001b[38;5;241m=\u001b[39mrandom_seed,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mcer_model,\n\u001b[1;32m     18\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/transformers/trainer.py:2768\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2768\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2771\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/transformers/trainer.py:2791\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2790\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1758\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1770\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1772\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:239\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    237\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m    238\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 239\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"~/temp/cer_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=False,\n",
    "    seed=random_seed,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=cer_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=cer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7982c37b-9bde-4bd1-8605-c1599de1df1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cer_model/tokenizer_config.json',\n",
       " 'cer_model/special_tokens_map.json',\n",
       " 'cer_model/vocab.txt',\n",
       " 'cer_model/added_tokens.json',\n",
       " 'cer_model/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"cer_model\")\n",
    "cer_tokenizer.save_pretrained(\"cer_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b60713-16ab-4016-a620-8b36a0cdce86",
   "metadata": {},
   "source": [
    "## 2.6 CER Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b3071dc-db59-4540-9e47-037cde8aea5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We can ignore the warning message.  This is simply due to the fact that\n",
    "# DebertaV2ForTokenClassification loads the DebertaV2 model first, then\n",
    "# initializes a random header model before restoring the states of the\n",
    "# TokenClassifer.  So we *do* have our fine-tuned model available.\n",
    "\n",
    "# if use_LoRA:\n",
    "#     config = PeftConfig.from_pretrained(\"cer_model\")\n",
    "\n",
    "#     cer_model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "#         pretrained_model_name_or_path=config.base_model_name_or_path,\n",
    "#         num_labels=3,\n",
    "#         id2label=id2label,\n",
    "#         label2id=label2id,\n",
    "#     )\n",
    "#     cer_model = PeftModel.from_pretrained(cer_model, \"cer_model\")\n",
    "# else:\n",
    "#     cer_model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "#         pretrained_model_name_or_path=\"cer_model\",\n",
    "#         num_labels=3,\n",
    "#         id2label=id2label,\n",
    "#         label2id=label2id,\n",
    "#     )\n",
    "\n",
    "if use_LoRA:\n",
    "    config = PeftConfig.from_pretrained(\"cer_model\")\n",
    "\n",
    "    cer_model = BertForTokenClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=config.base_model_name_or_path,\n",
    "        num_labels=3,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    cer_model = PeftModel.from_pretrained(cer_model, \"cer_model\")\n",
    "else:\n",
    "    cer_model = BertForTokenClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=\"cer_model\",\n",
    "        num_labels=3,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cdff28c-5c01-49a2-89ef-af6303408004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForTokenClassification' is not supported for token-classification. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'BrosForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FalconForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'MptForTokenClassification', 'MraForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'PhiForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n"
     ]
    }
   ],
   "source": [
    "# If using the adaptor, ignore the warning:\n",
    "# \"The model 'PeftModelForTokenClassification' is not supported for token-classification.\"\n",
    "# The PEFT model is wrapped just fine and will work within the pipeline.\n",
    "# N.B. moving model to CPU makes inference slower, but enables us to feed the pipeline\n",
    "# directly with strings.\n",
    "cer_pipeline = pipeline(\n",
    "    task=\"token-classification\",\n",
    "    model=cer_model,\n",
    "    tokenizer=cer_tokenizer,\n",
    "    aggregation_strategy=\"first\",\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20f97a67-e3ad-4b53-aa11-b3af5e19c63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tex2jax_ignore\" style=\"white-space: pre-wrap\"><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Name:  ___               Unit No:   ___</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Admission Date:  ___              Discharge Date:   ___</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Date of Birth:  ___             Sex:   M</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Service: ORTHOPAEDICS</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Allergies: </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">No Known Allergies</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> / </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Adverse Drug Reactions</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Attending: ___.</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Chief Complaint:</span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">R ankle</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">fracture</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">dislocation</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, open</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Major Surgical or Invasive Procedure:</span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">ORIF</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">R ankle</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> and </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">I&amp;D</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> ___</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">History of Present Illness:</span></div><div><span style=\"display: inline-block; vertical-align: top\">Chief Complaint:  </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">ankle pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\">Reason for Orthopedics Consult: management of </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">open fracture</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">HISTORY OF PRESENT ILLNESS: </span></div><div><span style=\"display: inline-block; vertical-align: top\">Patient is a ___ yo male previously healhty presenting w/ </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">fall</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">from 6 feet, from ladder. Patient landed on </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">LLE</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> w/ forced </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">eversion</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> and subsequent </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">open fracture</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">/</span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">dislocation</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. Denies </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">head </span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">strike</span></span><span style=\"display: inline-block; vertical-align: top\"> or </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">LOC</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. Denies </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">neck pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">back pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">chest pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">abd </span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">pain</span></span><span style=\"display: inline-block; vertical-align: top\">. Denies </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">pelvic</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> or </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">thigh pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. </span></div><div><span style=\"display: inline-block; vertical-align: top\">Was emergently reduced in ED under </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">conscious sedation</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. </span></div><div><span style=\"display: inline-block; vertical-align: top\">In the ED, initial </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">vitals</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> were 77 160/60 16 100%.  Per the ED, </span></div><div><span style=\"display: inline-block; vertical-align: top\">the patient&#x27;s exam did not show evidence of </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">neurovascular </span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">symptoms</span></span><span style=\"display: inline-block; vertical-align: top\">.</span></div><div><span style=\"display: inline-block; vertical-align: top\">Review of systems:  </span></div><div><span style=\"display: inline-block; vertical-align: top\">(+) Per HPI  </span></div><div><span style=\"display: inline-block; vertical-align: top\">(-) Denies </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">fever</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">chills</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">night sweats</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">recent weight loss</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> or </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">gain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. Denies </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">headache</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">neck</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> or </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">back pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Denies cough</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">shortness of breath</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">chest pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. Denies </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">nausea</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">vomiting</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">diarrhea</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">abdominal pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, or </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">changes in bowel habits</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. Denies </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">dysuria</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">frequency</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, or </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">urgency</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. </span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">PAST MEDICAL HISTORY:  </span></div><div><span style=\"display: inline-block; vertical-align: top\">none</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">MEDICATIONS:  </span></div><div><span style=\"display: inline-block; vertical-align: top\">none</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">ALLERGIES:  </span></div><div><span style=\"display: inline-block; vertical-align: top\">NKDA</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">SOCIAL HISTORY:  </span></div><div><span style=\"display: inline-block; vertical-align: top\">Denies alcohol, drugs, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">smoking</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">PHYSICAL EXAM:  </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">GENERAL</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Alert</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">oriented</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, no acute </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">distress</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">  </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">HEENT</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Sclera anicteric</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">MMM</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">oropharynx</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> clear  </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">NECK</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">C-spine</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> is </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">non-tender</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> to </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">palpation</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">LUNGS</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Clear to auscultation bilaterally</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">CV</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Regular rate and rhythm</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">ABD</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">soft</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">non-tender</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">non-distended</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">PELVIS</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: stable</span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">EXT</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">open fracture</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">/likely </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">dislocation</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> of </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">LLE</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> at level of </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">distal </span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">tibia</span></span><span style=\"display: inline-block; vertical-align: top\">. +</span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">DP</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. Unable to assess.  </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Warm</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">well perfused</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">2+ pulses</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span></div><div><span style=\"display: inline-block; vertical-align: top\">no </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">clubbing</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">cyanosis</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> or </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">edema</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. ___</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Labs: pending</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Images:  </span></div><div><span style=\"display: inline-block; vertical-align: top\">  </span></div><div><span style=\"display: inline-block; vertical-align: top\">ASSESSMENT &amp; PLAN:  </span></div><div><span style=\"display: inline-block; vertical-align: top\">___ yo male w/ type II </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">open fracture</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">/</span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">dislocation</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> of </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">distal </span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">tib</span></span><span style=\"display: inline-block; vertical-align: top\">/</span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">fib</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">.  </span></div><div><span style=\"display: inline-block; vertical-align: top\">1. Ancef 2g, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">tetanus</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\">2. </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Imaging</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\">3. Admit to ___ for </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">surgical repair</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\">4. Preop labs</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Past Medical History:</span></div><div><span style=\"display: inline-block; vertical-align: top\">none</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Social History:</span></div><div><span style=\"display: inline-block; vertical-align: top\">___</span></div><div><span style=\"display: inline-block; vertical-align: top\">Family History:</span></div><div><span style=\"display: inline-block; vertical-align: top\">not contributory</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Physical Exam:</span></div><div><span style=\"display: inline-block; vertical-align: top\">AF</span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">VS</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">S</span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">NAD</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">RLE</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">dressing</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> c/d/i</span></div><div><span style=\"display: inline-block; vertical-align: top\">___ intact dp/t</span></div><div><span style=\"display: inline-block; vertical-align: top\">___</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Brief Hospital Course:</span></div><div><span style=\"display: inline-block; vertical-align: top\">The patient presented to the emergency department and was </span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">evaluated by the orthopedic surgery team</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">. The patient was found </span></div><div><span style=\"display: inline-block; vertical-align: top\">to have </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">right ankle</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">open fracture</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">dislocation</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> and was </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">admitted </span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">to the orthopedic surgery service</span></span><span style=\"display: inline-block; vertical-align: top\">.  The patient was taken to the </span></div><div><span style=\"display: inline-block; vertical-align: top\">operating room on ___ for </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">R ankle</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">I&amp;D</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> and </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">ORIF</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, which the </span></div><div><span style=\"display: inline-block; vertical-align: top\">patient tolerated well (for full details please see the </span></div><div><span style=\"display: inline-block; vertical-align: top\">separately dictated </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">operative</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> report). The patient was taken </span></div><div><span style=\"display: inline-block; vertical-align: top\">from the OR to the </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">PACU</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> in </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">stable condition</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> and after recovery </span></div><div><span style=\"display: inline-block; vertical-align: top\">from </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">anesthesia</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> was transferred to the floor.  The patient was </span></div><div><span style=\"display: inline-block; vertical-align: top\">initially </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">given IV fluids</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> and </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">IV</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">pain medications</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, and </span></div><div><span style=\"display: inline-block; vertical-align: top\">progressed to a </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">regular diet</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> and </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">oral medications</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> by POD#1. The </span></div><div><span style=\"display: inline-block; vertical-align: top\">patient was given perioperative </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">antibiotics</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> and </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">anticoagulation</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">per routine.  The patients home medications were continued </span></div><div><span style=\"display: inline-block; vertical-align: top\">throughout this hospitalization.  The patient worked with ___ who </span></div><div><span style=\"display: inline-block; vertical-align: top\">determined that discharge to home was appropriate. The ___ </span></div><div><span style=\"display: inline-block; vertical-align: top\">hospital course was otherwise </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">unremarkable</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">.</span></div><div><span style=\"display: inline-block; vertical-align: top\">At the time of discharge the patient was afebrile with </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">stable </span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">vital signs</span></span><span style=\"display: inline-block; vertical-align: top\"> that were within normal limits, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> was well </span></div><div><span style=\"display: inline-block; vertical-align: top\">controlled with </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">oral medications</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">incisions</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> were </span></div><div><span style=\"display: inline-block; vertical-align: top\">clean/dry/intact, and the patient was </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">voiding</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">/</span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">moving bowels</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">spontaneously. The patient is NWB in the </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">right lower extremity</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span></div><div><span style=\"display: inline-block; vertical-align: top\">and will be discharged on lovenox for </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">DVT</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">prophylaxis</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">.  The </span></div><div><span style=\"display: inline-block; vertical-align: top\">patient will follow up in two weeks per routine. A thorough </span></div><div><span style=\"display: inline-block; vertical-align: top\">discussion was had with the patient regarding the diagnosis and </span></div><div><span style=\"display: inline-block; vertical-align: top\">expected post-discharge course, and all questions were answered </span></div><div><span style=\"display: inline-block; vertical-align: top\">prior to discharge.</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Medications on Admission:</span></div><div><span style=\"display: inline-block; vertical-align: top\">none</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Discharge Medications:</span></div><div><span style=\"display: inline-block; vertical-align: top\">1. Acetaminophen 1000 mg PO Q8H </span></div><div><span style=\"display: inline-block; vertical-align: top\">2. Docusate Sodium 100 mg PO BID </span></div><div><span style=\"display: inline-block; vertical-align: top\">3. Enoxaparin Sodium 40 mg SC QHS </span></div><div><span style=\"display: inline-block; vertical-align: top\">Start: Today - ___, First Dose: Next Routine Administration </span></div><div><span style=\"display: inline-block; vertical-align: top\">Time </span></div><div><span style=\"display: inline-block; vertical-align: top\">RX *enoxaparin 40 mg/0.4 mL 1 syringe at bedtime Disp #*14 </span></div><div><span style=\"display: inline-block; vertical-align: top\">Syringe Refills:*0</span></div><div><span style=\"display: inline-block; vertical-align: top\">4. OxycoDONE (Immediate Release)  ___ mg PO Q4H:PRN pain </span></div><div><span style=\"display: inline-block; vertical-align: top\">RX *oxycodone 5 mg ___ tablet(s) by mouth q3hrs Disp #*80 Tablet </span></div><div><span style=\"display: inline-block; vertical-align: top\">Refills:*0</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Discharge Disposition:</span></div><div><span style=\"display: inline-block; vertical-align: top\">Home</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Discharge Diagnosis:</span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">R ankle</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">fracture</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">dislocation</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Discharge Condition:</span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Mental Status</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: Clear and coherent.</span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Level of Consciousness</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">: </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Alert</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> and interactive.</span></div><div><span style=\"display: inline-block; vertical-align: top\">Activity Status: </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Ambulatory - requires assistance or aid</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> (walker </span></div><div><span style=\"display: inline-block; vertical-align: top\">or cane).</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Discharge Instructions:</span></div><div><span style=\"display: inline-block; vertical-align: top\">MEDICATIONS:</span></div><div><span style=\"display: inline-block; vertical-align: top\">- Please take all medications as prescribed by your physicians </span></div><div><span style=\"display: inline-block; vertical-align: top\">at discharge.</span></div><div><span style=\"display: inline-block; vertical-align: top\">- Continue all home medications unless specifically instructed </span></div><div><span style=\"display: inline-block; vertical-align: top\">to stop by your surgeon.</span></div><div><span style=\"display: inline-block; vertical-align: top\">- </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Do not drink alcohol</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, drive a motor vehicle, or operate </span></div><div><span style=\"display: inline-block; vertical-align: top\">machinery while taking narcotic </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">pain relievers</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">.</span></div><div><span style=\"display: inline-block; vertical-align: top\">- Narcotic </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">pain relievers</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> can cause </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">constipation</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, so you should </span></div><div><span style=\"display: inline-block; vertical-align: top\">drink eight 8oz glasses of water daily and </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">take a stool softener</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">(colace) to prevent this side effect.</span></div><div><span style=\"display: inline-block; vertical-align: top\">ANTICOAGULATION:</span></div><div><span style=\"display: inline-block; vertical-align: top\">- Please take lovenox 40mg daily for 2 weeks</span></div><div><span style=\"display: inline-block; vertical-align: top\">WOUND CARE:</span></div><div><span style=\"display: inline-block; vertical-align: top\">- No baths or swimming for at least 4 weeks.</span></div><div><span style=\"display: inline-block; vertical-align: top\">- Any stitches or staples that need to be removed will be taken </span></div><div><span style=\"display: inline-block; vertical-align: top\">out at your 2-week follow up appointment.</span></div><div><span style=\"display: inline-block; vertical-align: top\">- No </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">dressing</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> is needed if </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">wound</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> continues to be non-</span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">draining</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">.</span></div><div><span style=\"display: inline-block; vertical-align: top\">- </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Splint</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> must be left on until follow up appointment unless </span></div><div><span style=\"display: inline-block; vertical-align: top\">otherwise instructed</span></div><div><span style=\"display: inline-block; vertical-align: top\">- Do NOT get splint wet</span></div><div><span style=\"display: inline-block; vertical-align: top\">ACTIVITY AND WEIGHT BEARING:</span></div><div><span style=\"display: inline-block; vertical-align: top\">NWB </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">R ankle</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Danger Signs:</span></div><div><span style=\"display: inline-block; vertical-align: top\">Please call your </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">PCP</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> or surgeon&#x27;s office and/or return to the </span></div><div><span style=\"display: inline-block; vertical-align: top\">emergency department if you experience any of the following:</span></div><div><span style=\"display: inline-block; vertical-align: top\">- Increasing </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> that is not controlled with </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">pain medications</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\">- Increasing </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">redness</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">swelling</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">drainage</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, or other concerning </span></div><div><span style=\"display: inline-block; vertical-align: top\">changes in your </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">incision</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\">- Persistent or increasing </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">numbness, tingling</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, or </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">loss of </span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\"></span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">sensation</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\">- </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Fever</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> &gt; 101.4</span></div><div><span style=\"display: inline-block; vertical-align: top\">- Shaking </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">chills</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\">- </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Chest pain</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\">- </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Shortness of breath</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"></span></div><div><span style=\"display: inline-block; vertical-align: top\">- </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">Nausea</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> or </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">vomiting</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\"> with an inability to keep food, </span><span style=\"display: inline-block; vertical-align: top; position: relative; margin-bottom: 11px\"><span style=\"border-bottom: 3px solid #90caf9; padding-bottom: 8px\">liquid</span><span style=\"font-size: 11px; line-height: 1; white-space: nowrap; text-shadow: 1px 1px 0px white; position: absolute; left: 0; bottom: -8px\">GT</span></span><span style=\"display: inline-block; vertical-align: top\">, </span></div><div><span style=\"display: inline-block; vertical-align: top\">medications down</span></div><div><span style=\"display: inline-block; vertical-align: top\">- Any other medical concerns</span></div><div><span style=\"display: inline-block; vertical-align: top\"> </span></div><div><span style=\"display: inline-block; vertical-align: top\">Followup Instructions:</span></div><div><span style=\"display: inline-block; vertical-align: top\">___</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the predicted clinical entities against the actual annotated entities.\n",
    "# N.B. only the first 512 tokens of the note will contain predicted spans.\n",
    "# Not run due to sensitivity of MIMIC-IV notes\n",
    "\n",
    "note_id = \"10807423-DS-19\"\n",
    "text = test_notes_df.loc[note_id].text\n",
    "\n",
    "# +1 to offset the [CLS] token which will have been added by the tokenizer\n",
    "predicted_annotations = [\n",
    "    (span[\"start\"] + 1, span[\"end\"], \"PRED\") for span in cer_pipeline(text)\n",
    "]\n",
    "\n",
    "gt_annotations = [\n",
    "    (row.start, row.end, \"GT\") for row in test_annotations_df.loc[note_id].itertuples()\n",
    "]\n",
    "\n",
    "show_span_line_markup(text, predicted_annotations + gt_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c5b4c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9fc3b3-0a0a-41af-85fd-59f6d10d6ced",
   "metadata": {},
   "source": [
    "# 3. Linking Model\n",
    "\n",
    "The second part of the Entity Linker is the Linking model.  This component is charged with selecting the concepts from the knowledge base that best match the detected entity.\n",
    "\n",
    "We will build a simple, multi-level indexer for the task, drawing upon an encoder-only transformer that has been fine-tuned across the SNOMED CT concepts.\n",
    "\n",
    "The first index will find the most similar entity seen during training.  The second will use the context surrounding the entity to find the most likely concept matching said entity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b7114-6ccb-4cfe-a60c-5edac4dd1233",
   "metadata": {},
   "source": [
    "## 3.1 Load the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f647b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "642a7df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1308376 terms and 1230261 relationships were found in the release.\n",
      "Creating Relationships...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd5c66f8b944fb5904dacecbe1b0f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1230261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Concepts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909c4924b9da4b67befce37931ba6ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/503728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept with SCTID 286794000 has no FSN. Using synonym 'Detained in hospital under the Mental Health Act 1983 (E&W)' instead.\n",
      "Concept with SCTID 286799005 has no FSN. Using synonym 'Detained in hospital under Section 2 of the MHA 1983' instead.\n",
      "Concept with SCTID 286803006 has no FSN. Using synonym 'S3 MHA - Detained in hospital under Section 3 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 286807007 has no FSN. Using synonym 'Detained under emergency powers in hospital under Section 4 of the Mental Health Act 1983' instead.\n",
      "Concept with SCTID 286808002 has no FSN. Using synonym 'Detained in hospital under Section 5 of the MHA 1983' instead.\n",
      "Concept with SCTID 286809005 has no FSN. Using synonym 'Detained in hospital under nurses' holding power under Section 5(4) of the Mental Health Act 1983' instead.\n",
      "Concept with SCTID 286810000 has no FSN. Using synonym 'Detained in hosp under Sectn 5(2) Ment Health Act 83 E&W' instead.\n",
      "Concept with SCTID 292871007 has no FSN. Using synonym 'Bismuth subnitrate & iodoform paste impreg gauze advers reac' instead.\n",
      "Concept with SCTID 298430001 has no FSN. Using synonym 'Increased active range cervical spine right side flexion' instead.\n",
      "Concept with SCTID 298431002 has no FSN. Using synonym 'Decreased active range of cervical spine right side flexion' instead.\n",
      "Concept with SCTID 298435006 has no FSN. Using synonym 'Increased passive range cervical spine right lateral flexion' instead.\n",
      "Concept with SCTID 298436007 has no FSN. Using synonym 'Decreased passive range of cervical spine right side flexion' instead.\n",
      "Concept with SCTID 298441004 has no FSN. Using synonym 'Increased active range cervical spine left lateral flexion' instead.\n",
      "Concept with SCTID 298442006 has no FSN. Using synonym 'Decreased active range of cervical spine left side flexion' instead.\n",
      "Concept with SCTID 298446009 has no FSN. Using synonym 'Increased passive range cervical spine left lateral flexion' instead.\n",
      "Concept with SCTID 298447000 has no FSN. Using synonym 'Decreased passive range of cervical spine left side flexion' instead.\n",
      "Concept with SCTID 298532007 has no FSN. Using synonym 'Increased active range of dorsal spine right lateral flexion' instead.\n",
      "Concept with SCTID 298533002 has no FSN. Using synonym 'Decreased active range thoracic spine right lateral flexion' instead.\n",
      "Concept with SCTID 298537001 has no FSN. Using synonym 'Increased passive range of thoracic spine right side flexion' instead.\n",
      "Concept with SCTID 298538006 has no FSN. Using synonym 'Decreased passive range thoracic spine right lateral flexion' instead.\n",
      "Concept with SCTID 298543004 has no FSN. Using synonym 'Increased active range of thoracic spine left side flexion' instead.\n",
      "Concept with SCTID 298544005 has no FSN. Using synonym 'Decreased active range of thoracic spine left side flexion' instead.\n",
      "Concept with SCTID 298548008 has no FSN. Using synonym 'Increased passive range of dorsal spine left side flexion' instead.\n",
      "Concept with SCTID 298549000 has no FSN. Using synonym 'Decreased passive range thoracic spine left lateral flexion' instead.\n",
      "Concept with SCTID 298577009 has no FSN. Using synonym 'Observation of sensation of musculoskeletal structure of dorsal spine' instead.\n",
      "Concept with SCTID 298631005 has no FSN. Using synonym 'Increased passive range lumbar spine right lateral flexion' instead.\n",
      "Concept with SCTID 298632003 has no FSN. Using synonym 'Decreased passive range of lumbar spine right side flexion' instead.\n",
      "Concept with SCTID 302305009 has no FSN. Using synonym 'S35 MHA - Detained in hospital under Section 35 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 302321004 has no FSN. Using synonym 'Detained in hospital under Section 36 of the MHA 1983' instead.\n",
      "Concept with SCTID 302327000 has no FSN. Using synonym 'Detained in hospital under Section 37 of the MHA 1983' instead.\n",
      "Concept with SCTID 302328005 has no FSN. Using synonym 'S38 MHA - Detained in hospital under Section 38 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 302346004 has no FSN. Using synonym 'Detained in hospital under Section 44 of the MHA 1983' instead.\n",
      "Concept with SCTID 302347008 has no FSN. Using synonym 'S46 MHA - Detained in hospital under Section 46 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 302348003 has no FSN. Using synonym 'Detained in hospital under Section 48 of the MHA 1983' instead.\n",
      "Concept with SCTID 302380001 has no FSN. Using synonym 'Sentenced prisoner detained in hospital under Section 47 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 302444000 has no FSN. Using synonym 'S41 MHA - Subject to restriction order under Section 41 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 302445004 has no FSN. Using synonym 'Subject to restriction order under Section 46 of MHA 1983' instead.\n",
      "Concept with SCTID 302446003 has no FSN. Using synonym 'S49 MHA - Subject to restriction order under Section 49 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 302988002 has no FSN. Using synonym 'Released from restriction order under Section 42 of MHA 1983' instead.\n",
      "Concept with SCTID 302991002 has no FSN. Using synonym 'S7 MHA - Received into guardianship under Section 7 of the Mental Health Act1983 (England and Wales)' instead.\n",
      "Concept with SCTID 302993004 has no FSN. Using synonym 'S37 MHA - Received into guardianship on court order under Section 37 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 302998008 has no FSN. Using synonym 'S17 MHA - Leave of absence granted by Responsible Medical Officer under Section 17 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 302999000 has no FSN. Using synonym 'Leave of absence granted by RMO under Section 41 MHA 1983' instead.\n",
      "Concept with SCTID 303006005 has no FSN. Using synonym 'Discharged from liability to detentn under Sect 23 MHA 1983' instead.\n",
      "Concept with SCTID 303007001 has no FSN. Using synonym 'S41 MHA - Discharged from liability to detention under Section 41 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 303008006 has no FSN. Using synonym 'Discharged from liability to detentn under Sect 49 MHA 1983' instead.\n",
      "Concept with SCTID 303014004 has no FSN. Using synonym 'Returned to hospital under Section 18 of the MHA 1983' instead.\n",
      "Concept with SCTID 303015003 has no FSN. Using synonym 'S18 MHA - Recalled from leave of absence under Section 18 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 303016002 has no FSN. Using synonym 'Recalled from leave of absence under Sect 41(3)(c) MHA 1983' instead.\n",
      "Concept with SCTID 303018001 has no FSN. Using synonym 'Recalled from conditnl dischrge under Sect 41(3)(d) MHA 1983' instead.\n",
      "Concept with SCTID 303021004 has no FSN. Using synonym 'Patient returnd from absent without leave under S88 MHA 1983' instead.\n",
      "Concept with SCTID 303022006 has no FSN. Using synonym 'S87 MHA - Patient returned from being absent without leave under Section 87 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 303023001 has no FSN. Using synonym 'Patient returnd from absent without leave under S89 MHA 1983' instead.\n",
      "Concept with SCTID 303024007 has no FSN. Using synonym 'S138 MHA - Patient retaken from escaping custody under Section 138 of the Mental Health Act 1983 (England and Wales)' instead.\n",
      "Concept with SCTID 303210002 has no FSN. Using synonym 'MHA - Subject to court of protection under the Mental Health Act 1983 (England and Wales)' instead.\n",
      "SNOMED graph has 361179 vertices and 1179749 edges\n"
     ]
    }
   ],
   "source": [
    "SG = SnomedGraph.from_rf2(\"data/SnomedCT_InternationalRF2_PRODUCTION_20230531T120000Z_Challenge_Edition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd46712-d04d-4e0b-a9ee-fc2e27822baa",
   "metadata": {},
   "source": [
    "To load from a SNOMED RF2 folder (like the edition provided for the challenge) use:\n",
    "\n",
    "```SG = SnomedGraph.from_rf2(\"SnomedCT_InternationalRF2_PRODUCTION_20230531T120000Z_Challenge_Edition\")```\n",
    "\n",
    "Here, we will load a previously constructed concept graph and filter to the concepts that were in scope of the annotation exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "178b9c70-60ef-4932-8628-05c73e7f2d03",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../snomed_graph/full_concept_graph.gml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m SG \u001b[38;5;241m=\u001b[39m \u001b[43mSnomedGraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_serialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../snomed_graph/full_concept_graph.gml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/snomed_el_baseline_model/snomed_graph.py:371\u001b[0m, in \u001b[0;36mSnomedGraph.from_serialized\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_serialized\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Load a SnomedGraph from a serialization.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        A SnomedGraph\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m            \n\u001b[0;32m--> 371\u001b[0m     G \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_gml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestringizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SnomedGraph(G)\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/networkx/utils/decorators.py:770\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[0;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, __wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 5:3\u001b[0m, in \u001b[0;36margmap_read_gml_1\u001b[0;34m(path, label, destringizer, backend, **backend_kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[0;32m~/pkgman/miniconda3/envs/snomed/lib/python3.10/site-packages/networkx/utils/decorators.py:193\u001b[0m, in \u001b[0;36mopen_file.<locals>._open_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# could be None, or a file handle, in which case the algorithm will deal with it\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fobj, \u001b[38;5;28;01mlambda\u001b[39;00m: fobj\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../snomed_graph/full_concept_graph.gml'"
     ]
    }
   ],
   "source": [
    "SG = SnomedGraph.from_serialized(\"../snomed_graph/full_concept_graph.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d083491e-93d4-45fb-8951-c9a406b5f781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219169 concepts have been selected.\n"
     ]
    }
   ],
   "source": [
    "# If we want to load all of the concepts that were in scope of the annotation exercise, it's this:\n",
    "concepts_in_scope = (\n",
    "    SG.get_descendants(71388002)\n",
    "    | SG.get_descendants(123037004)\n",
    "    | SG.get_descendants(404684003)\n",
    ")\n",
    "print(f\"{len(concepts_in_scope)} concepts have been selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f435176-2630-42cf-9875-5d2610900a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336 concepts have been selected.\n"
     ]
    }
   ],
   "source": [
    "# If we want to simply use concepts for which we have a training example, it's this:\n",
    "concepts_in_scope = [\n",
    "    SG.get_concept_details(a) for a in annotations_df.concept_id.unique()\n",
    "]\n",
    "\n",
    "print(f\"{len(concepts_in_scope)} concepts have been selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e530662-c438-4812-ae66-24392922f667",
   "metadata": {},
   "source": [
    "## 3.2 Fine-tune the Linker's Encoder\n",
    "\n",
    "To fine-tune the encoder, we'll collect each in-scope concept from SNOMED CT and generate a training example from each pairwise combination of synonyms.  We train with a multiple negative-rankings loss.  This calculates the distance between each example pair and also the distance between the first example in the pair and _all other_ first examples in the batch.  The loss is constructed from the ranking of these distances.  We want the distance between an example and itself to be the minimum of all distances in the batch.  This should result in an embedding in which synonyms for the SNOMED concepts are encoded into close proximity.\n",
    "\n",
    "Note that this is a relatively trivial exploitation of the SNOMED CT graph.  We could experiment with other ways to generate pairs too, for example: by generating pairs that consist of parent and child concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "465e5782-d5db-44bb-bd0d-3542777bdc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4521272de5674d5ab6ca690ef4fa2e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af42284e14954bfe948af76a5a575378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd02ad446da345e780cb9b3a2a23f569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbae43e82ea47319474aea64f1f0d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kb_model = SentenceTransformer(kb_embedding_model_id)\n",
    "\n",
    "kb_sft_examples = [\n",
    "    InputExample(texts=[syn1, syn2], label=1)\n",
    "    for concept in tqdm(concepts_in_scope)\n",
    "    for syn1, syn2 in combinations(concept.synonyms, 2)\n",
    "]\n",
    "\n",
    "kb_sft_dataloader = DataLoader(kb_sft_examples, shuffle=True, batch_size=32)\n",
    "\n",
    "kb_sft_loss = losses.ContrastiveLoss(kb_model)\n",
    "\n",
    "kb_model.fit(\n",
    "    train_objectives=[(kb_sft_dataloader, kb_sft_loss)],\n",
    "    epochs=2,\n",
    "    warmup_steps=100,\n",
    "    checkpoint_path=\"~/temp/ke_encoder\",\n",
    ")\n",
    "\n",
    "kb_model.save(\"kb_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38657f44-b085-4a1a-94d6-17ad055cd938",
   "metadata": {},
   "source": [
    "## 3.3 Construct the Linker\n",
    "\n",
    "The simplest linker would simply map an entity (as extracted by the CER model) to the associated concept in the training dataset.  Two problems with this approach present themselves:\n",
    "\n",
    "1. We might encounter entities that have not been seen during training.\n",
    "2. Some entities might be mapped to >1 concept.  Why would this happen?  Consider the entity \"ABD\".  This is an abbreviation for \"Acute behavioural disorder\".  However, it is also shorthand for \"Abdomen\".\n",
    "\n",
    "To resolve the first problem our linker keeps an index of entities seen during training.  At inference time, it selects the known entity that is closest to the entity it is presented with.  (This is the \"candidate generation\" step.)\n",
    "\n",
    "To resolve the second problem, the linker builds a \"second level\" index for each entity.  This second level index maps each occurance of an entity + its surrounding context to the SNOMED concept it was annotated with.  At inference time, we encode the \\[entity + context\\] and find the most similar result in the second level index.  We return the associated SCTID.  (This is the \"candidate selection\" step.)\n",
    "\n",
    "We perform a simple grid search over context window sizes.\n",
    "\n",
    "As a further enhancement, we not only train the linker using entities seen in the training dataset but also with all of the synonyms for the in-scope SNOMED concepts (here there is no \"context\" for each of the entities, so we simply use the entity as its own context.)  You can run an ablation experiment by not passing the Linker any SNOMED concepts—the performance will drop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2efd6288-c35c-4889-b7f9-fe0d3471a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linker:\n",
    "    def __init__(self, encoder, context_window_width=0):\n",
    "        self.encoder = encoder\n",
    "        self.entity_index = KeyedVectors(self.encoder[1].word_embedding_dimension)\n",
    "        self.context_index = dict()\n",
    "        self.history = dict()\n",
    "        self.context_window_width = context_window_width\n",
    "\n",
    "    def add_context(self, row):\n",
    "        window_start = max(0, row.start - self.context_window_width)\n",
    "        window_end = min(row.end + self.context_window_width, len(row.text))\n",
    "        return row.text[window_start:window_end]\n",
    "\n",
    "    def add_entity(self, row):\n",
    "        return row.text[row.start : row.end]\n",
    "\n",
    "    def fit(self, df=None, snomed_concepts=None):\n",
    "        # Create a map from the entities to the concepts and contexts in which they appear\n",
    "        if df is not None:\n",
    "            for row in df.itertuples():\n",
    "                entity = self.add_entity(row)\n",
    "                context = self.add_context(row)\n",
    "                map_ = self.history.get(entity, dict())\n",
    "                contexts = map_.get(row.concept_id, list())\n",
    "                contexts.append(context)\n",
    "                map_[row.concept_id] = contexts\n",
    "                self.history[entity] = map_\n",
    "\n",
    "        # Add SNOMED CT codes for lookup\n",
    "        if snomed_concepts is not None:\n",
    "            for c in snomed_concepts:\n",
    "                for syn in c.synonyms:\n",
    "                    map_ = self.history.get(syn, dict())\n",
    "                    contexts = map_.get(c.sctid, list())\n",
    "                    contexts.append(syn)\n",
    "                    map_[c.sctid] = contexts\n",
    "                    self.history[syn] = map_\n",
    "\n",
    "        # Create indexes to help disambiguate entities by their contexts\n",
    "        for entity, map_ in tqdm(self.history.items()):\n",
    "            keys = [\n",
    "                (concept_id, occurance)\n",
    "                for concept_id, contexts in map_.items()\n",
    "                for occurance, context in enumerate(contexts)\n",
    "            ]\n",
    "            contexts = [context for contexts in map_.values() for context in contexts]\n",
    "            vectors = self.encoder.encode(contexts)\n",
    "            index = KeyedVectors(self.encoder[1].word_embedding_dimension)\n",
    "            index.add_vectors(keys, vectors)\n",
    "            self.context_index[entity] = index\n",
    "\n",
    "        # Now create the top-level entity index\n",
    "        keys = list(self.history.keys())\n",
    "        vectors = self.encoder.encode(keys)\n",
    "        self.entity_index.add_vectors(keys, vectors)\n",
    "\n",
    "    def link(self, row):\n",
    "        entity = self.add_entity(row)\n",
    "        context = self.add_context(row)\n",
    "        vec = self.encoder.encode(entity)\n",
    "        nearest_entity = self.entity_index.most_similar(vec, topn=1)[0][0]\n",
    "        index = self.context_index.get(nearest_entity, None)\n",
    "\n",
    "        if index:\n",
    "            vec = self.encoder.encode(context)\n",
    "            key, score = index.most_similar(vec, topn=1)[0]\n",
    "            sctid, _ = key\n",
    "            return sctid\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a2d7a4c-2aaa-4da4-8a4d-bac0c57d27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "linker_training_df = training_notes_df.join(training_annotations_df)\n",
    "linker_test_df = test_notes_df.join(test_annotations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54ad2e8c-85db-47c8-8d09-7e10bd10b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_linker(linker, df):\n",
    "    n_correct = 0\n",
    "    n_examples = df.shape[0]\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=n_examples):\n",
    "        sctid = linker.link(row)\n",
    "        if row[\"concept_id\"] == sctid:\n",
    "            n_correct += 1\n",
    "\n",
    "    return n_correct / n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b969fc37-2dc8-4976-9b79-f250f5a9effe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86448aa2ae6453e8e7ec8d552dfaae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fbe30e3a1a4f8382dc9de934b12967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fbc7050e0f4ac9a76b45b82e90f2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window Width: 5\tAccuracy: 0.8453127083611148\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1985753ea5414c45a86a9226e49737eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac779bbc062468cbba2468f3aef56c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window Width: 8\tAccuracy: 0.8467795706094146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbecfdd1c88d4f7a895a6ec0dd58c99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e298731558477a87f034188cd025cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window Width: 10\tAccuracy: 0.849179890652087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6f911920eb45c38e284fde5ae6ba79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e825a120d8f4c8ebeb6113519beedd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window Width: 12\tAccuracy: 0.8481130817442326\n"
     ]
    }
   ],
   "source": [
    "for context_window_width in tqdm([5, 8, 10, 12]):\n",
    "    linker = Linker(kb_model, context_window_width)\n",
    "    linker.fit(linker_training_df, concepts_in_scope)\n",
    "    acc = evaluate_linker(linker, linker_test_df)\n",
    "    print(f\"Context Window Width: {context_window_width}\\tAccuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8abf7ee6-e89a-4f74-b043-a0bbeb186a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0f3ed8089641f48e9fc08281c93472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "linker = Linker(kb_model, 12)\n",
    "linker.fit(linker_training_df, concepts_in_scope)\n",
    "\n",
    "with open(\"linker.pickle\", \"wb\") as f:\n",
    "    pickle.dump(linker, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c81ef28-4bf6-4232-b081-9932eda39078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then re-load the linker with:\n",
    "with open(\"linker.pickle\", \"rb\") as f:\n",
    "    linker = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c7f6a-d395-4f00-ba2c-64e3d3167692",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "\n",
    "Here we glue the Clinical Entity Recogniser model to the Linker model and show how to generate and evaluate predictions over our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5123c61-35db-4110-a485-0d7737f403a8",
   "metadata": {},
   "source": [
    "## 4.1 Prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4aea5c14-ea3a-4e54-b9b0-aeafe1993269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b5f3a63d25413c98122be638d251e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willh/venvs/snomed/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:393: UserWarning: Tokenizer does not support real words, using fallback heuristic\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def predict(df):\n",
    "    # One note at a time...\n",
    "    for row in tqdm(df.itertuples(), total=df.shape[0]):\n",
    "        # Tokenize the entire discharge note\n",
    "        tokenized = cer_tokenizer(\n",
    "            row.text,\n",
    "            return_offsets_mapping=False,\n",
    "            add_special_tokens=False,\n",
    "            truncation=False,\n",
    "        )\n",
    "\n",
    "        global_offset = 0\n",
    "        global_offset_mapping = []\n",
    "\n",
    "        # Adjust the token offsets so that they match the original document\n",
    "        for input_id in tokenized[\"input_ids\"]:\n",
    "            token = cer_tokenizer.decode(input_id)\n",
    "            pos = row.text[global_offset:].find(token)\n",
    "            start = global_offset + pos\n",
    "            end = global_offset + pos + len(token)\n",
    "            global_offset = end\n",
    "            global_offset_mapping.append((start, end))\n",
    "\n",
    "        chunk_start_idx = 0\n",
    "        chunk_end_idx = 0\n",
    "\n",
    "        # Process the document in chunks of 512 tokens chunk at a time\n",
    "        for offset_chunk in chunked(global_offset_mapping, max_seq_len - 2):\n",
    "            chunk_start_idx = chunk_end_idx\n",
    "            chunk_end_idx = offset_chunk[-1][1]\n",
    "            chunk_text = row.text[chunk_start_idx:chunk_end_idx]\n",
    "\n",
    "            # Iterate through the detected entities and link them\n",
    "            for entity in cer_pipeline(chunk_text):\n",
    "                example = pd.Series(\n",
    "                    {\n",
    "                        # +1 to account for the [CLS] token\n",
    "                        \"start\": entity[\"start\"] + chunk_start_idx + 1,\n",
    "                        \"end\": entity[\"end\"] + chunk_start_idx,\n",
    "                        \"text\": row.text,\n",
    "                    }\n",
    "                )\n",
    "                sctid = linker.link(example)\n",
    "\n",
    "                # Only yield matches where the Linker returned something\n",
    "                if sctid:\n",
    "                    yield {\n",
    "                        \"note_id\": row.Index,\n",
    "                        \"start\": example[\"start\"],\n",
    "                        \"end\": example[\"end\"],\n",
    "                        \"concept_id\": sctid,\n",
    "                        # The following are useful for debugging and analysis\n",
    "                        \"FSN\": SG.get_concept_details(sctid).fsn,\n",
    "                        \"entity\": row.text[example[\"start\"] : example[\"end\"]],\n",
    "                        \"tokenizer_word\": entity[\"word\"],\n",
    "                    }\n",
    "\n",
    "\n",
    "preds_df = pd.DataFrame(list(predict(test_notes_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba937fd-5059-47a1-8831-638a3820bf5c",
   "metadata": {},
   "source": [
    "## 4.3 Visualisation\n",
    "\n",
    "The following code will compare the ground truth (\"GT_\") annotations to the predicted (\"P_\") annotations.  Since we cannot share the text of these notes, the outputs of this code have been hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a7017-cf34-496d-bb94-ea58902184ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_id = \"10807423-DS-19\"\n",
    "text = test_notes_df.loc[note_id].text\n",
    "\n",
    "predicted_annotations = [\n",
    "    (row.start, row.end, f\"P_{row.concept_id}\")\n",
    "    for row in preds_df.set_index(\"note_id\").loc[note_id].itertuples()\n",
    "]\n",
    "\n",
    "gt_annotations = [\n",
    "    (row.start, row.end, f\"GT_{row.concept_id}\")\n",
    "    for row in test_annotations_df.loc[note_id].itertuples()\n",
    "]\n",
    "\n",
    "show_span_line_markup(text, predicted_annotations + gt_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd7182-f98b-459a-af1f-57d4885e909f",
   "metadata": {},
   "source": [
    "## 4.3 Scoring\n",
    "\n",
    "We apply a token-level scorer function, which is what the competition will use to evaluate solutions.  We run this over our reserved test set to get a sense for out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47a8cd6d-808c-4af2-a137-2e81097ef10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_per_class(\n",
    "    user_annotations: pd.DataFrame, target_annotations: pd.DataFrame\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculate the IoU metric for each class in a set of annotations.\n",
    "    \"\"\"\n",
    "    # Get mapping from note_id to index in array\n",
    "    docs = np.unique(\n",
    "        np.concatenate([user_annotations.note_id, target_annotations.note_id])\n",
    "    )\n",
    "    doc_index_mapping = dict(zip(docs, range(len(docs))))\n",
    "\n",
    "    # Identify union of categories in GT and PRED\n",
    "    cats = np.unique(\n",
    "        np.concatenate([user_annotations.concept_id, target_annotations.concept_id])\n",
    "    )\n",
    "\n",
    "    # Find max character index in GT or PRED\n",
    "    max_end = np.max(np.concatenate([user_annotations.end, target_annotations.end]))\n",
    "\n",
    "    # Populate matrices for keeping track of character class categorization\n",
    "    def populate_char_mtx(n_rows, n_cols, annot_df):\n",
    "        mtx = sp.lil_array((n_rows, n_cols), dtype=np.uint64)\n",
    "        for row in annot_df.itertuples():\n",
    "            doc_index = doc_index_mapping[row.note_id]\n",
    "            mtx[doc_index, row.start : row.end] = row.concept_id  # noqa: E203\n",
    "        return mtx.tocsr()\n",
    "\n",
    "    gt_mtx = populate_char_mtx(docs.shape[0], max_end, target_annotations)\n",
    "    pred_mtx = populate_char_mtx(docs.shape[0], max_end, user_annotations)\n",
    "\n",
    "    # Calculate IoU per category\n",
    "    ious = []\n",
    "    for cat in cats:\n",
    "        gt_cat = gt_mtx == cat\n",
    "        pred_cat = pred_mtx == cat\n",
    "        # sparse matrices don't support bitwise operators, but the _cat matrices\n",
    "        # have bool dtypes so when we multiply/add them we end up with only T/F values\n",
    "        intersection = gt_cat * pred_cat\n",
    "        union = gt_cat + pred_cat\n",
    "        iou = intersection.sum() / union.sum()\n",
    "        ious.append(iou)\n",
    "\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1ce6058-18c0-4596-a9d3-c49ac2dafd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-averaged character IoU metric: 0.2584\n"
     ]
    }
   ],
   "source": [
    "ious = iou_per_class(preds_df, test_annotations_df.reset_index())\n",
    "print(f\"macro-averaged character IoU metric: {np.mean(ious):0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e36e2e-2783-496d-a89a-a6f8fbd36548",
   "metadata": {},
   "source": [
    "# 5. Preparing for Submission\n",
    "\n",
    "Here we wrap the model up into a compliant submission format. (Note that, before submitting, we'd want to re-fit both the CER model (using the optimal number of training epochs) and the Linker on _all_ of the data.) Here, we'll just re-train briefly on the held-out notes and annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503fadf-1d7c-467c-aa94-e0dbb48056fe",
   "metadata": {},
   "source": [
    "## 5.1 Finalise the CER model\n",
    "\n",
    "We'll give a final epoch of supervised fine-tuning over the held-out notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9937cc3e-d85d-4fa6-b7a2-2c50084b7f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 04:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.136878</td>\n",
       "      <td>0.816864</td>\n",
       "      <td>0.852457</td>\n",
       "      <td>0.834281</td>\n",
       "      <td>0.948601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 1\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=cer_model,\n",
    "    args=training_args,\n",
    "    train_dataset=test,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=cer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"cer_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b7384-4aa8-4e26-a76e-3830797fa1a7",
   "metadata": {},
   "source": [
    "## 5.2 Finalise the Linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d0dbc5a-a5a5-42c2-9f18-6b02417089a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db95bcce61614ad1812eb0d87204abd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kb_model = SentenceTransformer(\"kb_model\")\n",
    "linker = Linker(kb_model, 12)\n",
    "linker.fit(notes_df.join(annotations_df), concepts_in_scope)\n",
    "\n",
    "with open(\"linker.pickle\", \"wb\") as f:\n",
    "    pickle.dump(linker, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16181488-8a0b-412e-966e-69fdf67a44ec",
   "metadata": {},
   "source": [
    "The contents of `main.py` for a submission that complies with the [runtime specification](https://github.com/drivendataorg/snomed-ct-entity-linking-runtime/tree/main) are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78967b52-16ff-4a25-a744-1cf5f95c7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Benchmark submission for Entity Linking Challenge.\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "import dill as pickle\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from more_itertools import chunked\n",
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoTokenizer, DebertaV2ForTokenClassification, pipeline\n",
    "\n",
    "NOTES_PATH = Path(\"data/test_notes.csv\")\n",
    "SUBMISSION_PATH = Path(\"submission.csv\")\n",
    "LINKER_PATH = Path(\"linker.pickle\")\n",
    "CER_MODEL_PATH = Path(\"cer_model\")\n",
    "\n",
    "CONTEXT_WINDOW_WIDTH = 12\n",
    "MAX_SEQ_LEN = 512\n",
    "USE_LORA = False\n",
    "\n",
    "\n",
    "def load_cer_pipeline():\n",
    "    label2id = {\"O\": 0, \"B-clinical_entity\": 1, \"I-clinical_entity\": 2}\n",
    "\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "    cer_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CER_MODEL_PATH, model_max_length=MAX_SEQ_LEN\n",
    "    )\n",
    "\n",
    "    if USE_LORA:\n",
    "        config = PeftConfig.from_pretrained(CER_MODEL_PATH)\n",
    "\n",
    "        cer_model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=config.base_model_name_or_path,\n",
    "            num_labels=3,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "        )\n",
    "        cer_model = PeftModel.from_pretrained(cer_model, CER_MODEL_PATH)\n",
    "    else:\n",
    "        cer_model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=CER_MODEL_PATH,\n",
    "            num_labels=3,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "        )\n",
    "\n",
    "    cer_pipeline = pipeline(\n",
    "        task=\"token-classification\",\n",
    "        model=cer_model,\n",
    "        tokenizer=cer_tokenizer,\n",
    "        aggregation_strategy=\"first\",\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    return cer_pipeline\n",
    "\n",
    "\n",
    "def main():\n",
    "    # columns are note_id, text\n",
    "    logger.info(\"Reading in notes data.\")\n",
    "    notes = pd.read_csv(NOTES_PATH)\n",
    "    logger.info(f\"Found {notes.shape[0]} notes.\")\n",
    "    spans = []\n",
    "\n",
    "    # Load model components\n",
    "    logger.info(\"Loading CER pipeline.\")\n",
    "    cer_pipeline = load_cer_pipeline()\n",
    "    cer_tokenizer = cer_pipeline.tokenizer\n",
    "\n",
    "    logger.info(\"Loading Linker\")\n",
    "    with open(LINKER_PATH, \"rb\") as f:\n",
    "        linker = pickle.load(f)\n",
    "\n",
    "    # Process one note at a time...\n",
    "    logger.info(\"Processing notes.\")\n",
    "    for row in notes.itertuples():\n",
    "        # Tokenize the entire discharge note\n",
    "        tokenized = cer_tokenizer(\n",
    "            row.text,\n",
    "            return_offsets_mapping=False,\n",
    "            add_special_tokens=False,\n",
    "            truncation=False,\n",
    "        )\n",
    "\n",
    "        global_offset = 0\n",
    "        global_offset_mapping = []\n",
    "\n",
    "        # Adjust the token offsets so that they match the original document\n",
    "        for input_id in tokenized[\"input_ids\"]:\n",
    "            token = cer_tokenizer.decode(input_id)\n",
    "            pos = row.text[global_offset:].find(token)\n",
    "            start = global_offset + pos\n",
    "            end = global_offset + pos + len(token)\n",
    "            global_offset = end\n",
    "            global_offset_mapping.append((start, end))\n",
    "\n",
    "        chunk_start_idx = 0\n",
    "        chunk_end_idx = 0\n",
    "\n",
    "        # Process the document in chunks of 512 tokens chunk at a time\n",
    "        for offset_chunk in chunked(global_offset_mapping, MAX_SEQ_LEN - 2):\n",
    "            chunk_start_idx = chunk_end_idx\n",
    "            chunk_end_idx = offset_chunk[-1][1]\n",
    "            chunk_text = row.text[chunk_start_idx:chunk_end_idx]\n",
    "\n",
    "            # ...one matched clinical entity at a time\n",
    "            # Iterate through the detected entities and link them\n",
    "            for entity in cer_pipeline(chunk_text):\n",
    "                example = pd.Series(\n",
    "                    {\n",
    "                        # +1 to account for the [CLS] token\n",
    "                        \"start\": entity[\"start\"] + chunk_start_idx + 1,\n",
    "                        \"end\": entity[\"end\"] + chunk_start_idx,\n",
    "                        \"text\": row.text,\n",
    "                    }\n",
    "                )\n",
    "                sctid = linker.link(example)\n",
    "                if sctid:\n",
    "                    spans.append(\n",
    "                        {\n",
    "                            \"note_id\": row.Index,\n",
    "                            \"start\": example[\"start\"],\n",
    "                            \"end\": example[\"end\"],\n",
    "                            \"concept_id\": sctid,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    logger.info(f\"Generated {len(spans)} annotated spans.\")\n",
    "    spans_df = pd.DataFrame(spans)\n",
    "    spans_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    logger.info(\"Finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6229c",
   "metadata": {},
   "source": [
    "To create a valid submission, follow the instructions in the competition [runtime repository](https://github.com/drivendataorg/snomed-ct-entity-linking-runtime/tree/main). You would clone the runtime repo, copy `main.py` as well as the `cer_model/` folder and the `linker.pickle` file from this repo into the `submission_src` folder in the runtime repo and then run `make pack-submission` to generate a submission zip file. You could also follow the runtime repo instructions to generate smoke test data (`make smoke-test-data`) so you can test how your submission performs locally (`make test-submission`) before submitting to the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe864ee-e7aa-4f2b-ae8a-48a2e0689bca",
   "metadata": {},
   "source": [
    "# Parting Words\n",
    "\n",
    "There's a fair amount that goes into an entity linker.  The approach we took here - using transformer encoders - has the virtue of being quick to fine-tune and easy to experiment with; on the flip-side, it's difficult to get good performance from a 300M parameter encoder for the CER step using \"out of the box\" fine-tuning.  Furthermore, the requirement to chunk the documents and align the annotations with the tokenization scheme adds unwelcome complexity to the code. Entity linkers that use modern, decoder-based transformers - having the virtue of longer context windows and a deeper \"understanding\" of natural language - should be able to beat this benchmark.\n",
    "\n",
    "Furthermore, the model constructed in notebook does not take full advantage of the knowledge encoded within the SNOMED Clinical Terminology.  We used synonyms to fine-tune the Knowledge Base Encoder but made no use of either the hierarchy or the defining relationships in constructing fine-tuning datasets. For example, in a decoder-based model, we can imagine developing _retrieval augmented generation_ techniques for candidate selection.\n",
    "\n",
    "The full power of SNOMED CT is an underexplored area for the development of Clinical Entity Linking models.  We wish you all the best in your experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096eea7-4a30-47d9-b14d-4a62d8a0f855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
